{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ELqZaWJVugvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b842042-9056-4e18-8725-e586fb043740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit>=1.20.0 tensorflow>=2.10.0 transformers>=4.26.0 emoji>=2.2.0 pyngrok>=5.1.0"
      ],
      "metadata": {
        "id": "KcMFLzZR6HtT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#  Create the Streamlit app file\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Multilingual Hate Speech Detector\",\n",
        "    page_icon=\"üõ°Ô∏è\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    \"\"\"Load the model and tokenizer with caching to improve performance\"\"\"\n",
        "    # Path to your saved model in Google Drive\n",
        "    model_path = \"/content/drive/MyDrive/ProjetNLP/best_multilingual_hate_speech_model.pt\"\n",
        "\n",
        "    # Use RoBERTa tokenizer instead of BERT\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "    try:\n",
        "        # Initialize a RoBERTa model\n",
        "        model = RobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
        "\n",
        "        # Load the state dict from the .pt file\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "        model.eval()  # Set to evaluation mode\n",
        "        st.success(\"Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}\")\n",
        "        st.error(\"Make sure your model is saved at '/content/drive/MyDrive/ProjetNLP/best_multilingual_hate_speech_model.pt'\")\n",
        "\n",
        "        # Additional information for debugging\n",
        "        st.error(\"Your model appears to be a RoBERTa model. Make sure you're using the correct model architecture.\")\n",
        "        raise e\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean the input text similar to training preprocessing\"\"\"\n",
        "    # Remove mentions\n",
        "    text = re.sub(r\"@[\\d\\w_]+\\s?\", \"\", text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"\", text)\n",
        "    # Remove Twitter image URLs\n",
        "    text = re.sub(r\"pic.twitter.com/[\\w\\d]+\", \"\", text)\n",
        "    # Convert emojis to text\n",
        "    text = emoji.demojize(text)\n",
        "    return text\n",
        "\n",
        "def predict_hate_speech(text, model, tokenizer):\n",
        "    \"\"\"Make prediction on whether input text contains hate speech\"\"\"\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(text)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        cleaned_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"  # PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():  # No gradient computation needed for inference\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=1).numpy()[0]\n",
        "\n",
        "    # Get prediction (0 = not hate speech, 1 = hate speech)\n",
        "    prediction = int(probabilities[1] > 0.5)  # Use threshold\n",
        "    confidence = float(probabilities[prediction])\n",
        "\n",
        "    return {\n",
        "        \"is_hate_speech\": bool(prediction),\n",
        "        \"confidence\": confidence,\n",
        "        \"cleaned_text\": cleaned_text,\n",
        "        \"probabilities\": {\n",
        "            \"not_hate_speech\": float(probabilities[0]),\n",
        "            \"hate_speech\": float(probabilities[1])\n",
        "        }\n",
        "    }\n",
        "\n",
        "def get_token_contributions(text, model, tokenizer):\n",
        "    \"\"\"Analyze which words contribute most to the hate speech classification\"\"\"\n",
        "    cleaned_text = preprocess_text(text)\n",
        "    tokens = tokenizer.tokenize(cleaned_text)\n",
        "\n",
        "    # Get baseline prediction for the full text\n",
        "    baseline_result = predict_hate_speech(cleaned_text, model, tokenizer)\n",
        "    baseline_prediction = baseline_result[\"is_hate_speech\"]\n",
        "\n",
        "    # If there are too many tokens, focus on a subset\n",
        "    max_tokens_to_analyze = min(len(tokens), 25)  # Reduced for Colab performance\n",
        "    tokens_to_analyze = tokens[:max_tokens_to_analyze]\n",
        "\n",
        "    token_impacts = []\n",
        "\n",
        "    # For each token, measure the impact of removing it\n",
        "    for i, token in enumerate(tokens_to_analyze):\n",
        "        if token.startswith('ƒ†') or token in ['<s>', '</s>', '.', ',', '!', '?']:\n",
        "            continue  # Skip special tokens in RoBERTa\n",
        "\n",
        "        # Create a version of the text without this token\n",
        "        modified_tokens = tokens.copy()\n",
        "        modified_tokens[i] = \"<mask>\"  # RoBERTa uses <mask> instead of [MASK]\n",
        "        modified_text = tokenizer.convert_tokens_to_string(modified_tokens)\n",
        "\n",
        "        # Predict on the modified text\n",
        "        result = predict_hate_speech(modified_text, model, tokenizer)\n",
        "\n",
        "        # Calculate the impact of this token\n",
        "        impact = 0\n",
        "        if baseline_prediction:  # If original text is classified as hate speech\n",
        "            # Impact is positive if removing the token reduces the hate probability\n",
        "            impact = baseline_result[\"confidence\"] - result[\"confidence\"]\n",
        "        else:  # If original text is not classified as hate speech\n",
        "            # Impact is positive if removing the token increases the non-hate probability\n",
        "            impact = result[\"confidence\"] - baseline_result[\"confidence\"]\n",
        "\n",
        "        token_impacts.append({\n",
        "            \"token\": token,\n",
        "            \"impact\": impact\n",
        "        })\n",
        "\n",
        "    # Sort by absolute impact\n",
        "    token_impacts.sort(key=lambda x: abs(x[\"impact\"]), reverse=True)\n",
        "\n",
        "    return token_impacts[:5]  # Return top 5 impactful tokens\n",
        "\n",
        "def main():\n",
        "    st.title(\"Multilingual Hate Speech Detector\")\n",
        "    st.markdown(\"\"\"\n",
        "    This application detects hate speech in multiple languages using a fine-tuned XLM-RoBERTa model.\n",
        "    Enter text in any language and the model will analyze it for hate speech content.\n",
        "    \"\"\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    with st.spinner(\"Loading model... This may take a moment.\"):\n",
        "        try:\n",
        "            model, tokenizer = load_model_and_tokenizer()\n",
        "        except Exception as e:\n",
        "            st.error(\"Failed to load the model. Check the error message above.\")\n",
        "            return\n",
        "\n",
        "    # Text input\n",
        "    text_input = st.text_area(\"Enter text to analyze:\", height=150)\n",
        "\n",
        "    analyze_button = st.button(\"Analyze Text\")\n",
        "\n",
        "    if analyze_button:\n",
        "        if not text_input:\n",
        "            st.warning(\"Please enter some text to analyze.\")\n",
        "        else:\n",
        "            with st.spinner(\"Analyzing text...\"):\n",
        "                try:\n",
        "                    # Make prediction\n",
        "                    result = predict_hate_speech(text_input, model, tokenizer)\n",
        "\n",
        "                    # Display results\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        if result[\"is_hate_speech\"]:\n",
        "                            st.error(f\"‚ö†Ô∏è Hate speech detected with {result['probabilities']['hate_speech']*100:.2f}% confidence\")\n",
        "                        else:\n",
        "                            st.success(f\"‚úÖ No hate speech detected with {result['probabilities']['not_hate_speech']*100:.2f}% confidence\")\n",
        "\n",
        "                        # Display probability bars\n",
        "                        st.markdown(\"### Probability Analysis\")\n",
        "                        st.markdown(\"**Not Hate Speech:**\")\n",
        "                        st.progress(float(result['probabilities']['not_hate_speech']))\n",
        "                        st.markdown(f\"{result['probabilities']['not_hate_speech']*100:.2f}%\")\n",
        "\n",
        "                        st.markdown(\"**Hate Speech:**\")\n",
        "                        st.progress(float(result['probabilities']['hate_speech']))\n",
        "                        st.markdown(f\"{result['probabilities']['hate_speech']*100:.2f}%\")\n",
        "\n",
        "                    # Get and display token contributions\n",
        "                    with col2:\n",
        "                        st.subheader(\"Word Impact Analysis\")\n",
        "                        try:\n",
        "                            token_impacts = get_token_contributions(text_input, model, tokenizer)\n",
        "\n",
        "                            if token_impacts:\n",
        "                                st.markdown(\"These words had the most impact on the classification:\")\n",
        "                                for item in token_impacts:\n",
        "                                    impact_direction = \"increases\" if item[\"impact\"] > 0 else \"decreases\"\n",
        "                                    st.markdown(f\"- **{item['token'].replace('ƒ†', '')}**: {impact_direction} hate speech probability by {abs(item['impact']*100):.2f}%\")\n",
        "                            else:\n",
        "                                st.info(\"No significant word impacts found\")\n",
        "                        except Exception as word_error:\n",
        "                            st.warning(f\"Could not perform word impact analysis: {word_error}\")\n",
        "\n",
        "                    # Display the processed text\n",
        "                    st.subheader(\"Processed Text\")\n",
        "                    st.text(result[\"cleaned_text\"])\n",
        "\n",
        "                except Exception as analysis_error:\n",
        "                    st.error(f\"Error analyzing text: {analysis_error}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "khI2B_WPG1RQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b146af2-4522-4abe-ec69-0adf0c9a76dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3awbpiti3f9",
        "outputId": "367914ec-7878-47eb-f673-0c47e82f1736"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.201.151.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeRngQKEvTpn",
        "outputId": "d6e9474d-fe04-48df-bde4-d88c686bb12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.201.151.4:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0Kyour url is: https://lucky-horses-throw.loca.lt\n",
            "2025-03-04 07:47:59.338438: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741074479.368611    4967 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741074479.377396    4967 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-04 07:47:59.406675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 168kB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 3.99MB/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 11.9MB/s]\n",
            "tokenizer.json: 100% 9.10M/9.10M [00:00<00:00, 18.4MB/s]\n",
            "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
            "model.safetensors: 100% 1.12G/1.12G [00:05<00:00, 195MB/s]\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/content/app.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
            "2025-03-04 07:48:39.950 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 345, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.load('/content/drive/MyDrive/ProjetNLP/best_multilingual_hate_speech_model.pt')\n",
        "print(type(model))\n",
        "# If it's a dict, print the keys\n",
        "if isinstance(model, dict):\n",
        "    print(list(model.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGNJDojX0sCG",
        "outputId": "832cfd32-86d6-4375-9a6f-9d773e1ea3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-37acd57a2bf7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('/content/drive/MyDrive/ProjetNLP/best_multilingual_hate_speech_model.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'collections.OrderedDict'>\n",
            "['roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDFI3hNWuIch",
        "outputId": "69a2279c-36d7-46a1-c3e9-590d24c28b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    }
  ]
}